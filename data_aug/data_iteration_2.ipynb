{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Augmentation Iteration 2"
      ],
      "metadata": {
        "id": "FkTKAiPlQPqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install opendatasets"
      ],
      "metadata": {
        "id": "jQkasVYJQU0Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dilICAiXPViB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b378f2fa-0ed9-4960-80c7-b33f4f18f2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opendatasets\n",
            "  Downloading opendatasets-0.1.22-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.7.4.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.1.31)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (0.5.1)\n",
            "Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: opendatasets\n",
            "Successfully installed opendatasets-0.1.22\n"
          ]
        }
      ],
      "source": [
        "!pip install opendatasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Open the dataset from Kaggle. Requires username and key, see README.md for more details"
      ],
      "metadata": {
        "id": "rrkOKhKjQZI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset/data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGjC989gPpvu",
        "outputId": "ad8264cd-f45c-476f-e2dd-6efb875306f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: johnnyplays\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/jonathanoheix/face-expression-recognition-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete redundant image folder"
      ],
      "metadata": {
        "id": "RymP9jUBQsfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"/content/face-expression-recognition-dataset/images/images\")"
      ],
      "metadata": {
        "id": "b9jCRjRePqqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Establish training and testing set"
      ],
      "metadata": {
        "id": "N73ipQXRQv2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = \"/content/face-expression-recognition-dataset/images/train\"\n",
        "test_set = \"/content/face-expression-recognition-dataset/images/validation\""
      ],
      "metadata": {
        "id": "xqGIJMPjPuG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to create dataframe"
      ],
      "metadata": {
        "id": "ydcC6wmlQyY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dataframe(direct):\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for label in os.listdir(direct):\n",
        "        for filename in os.listdir(os.path.join(direct, label)):\n",
        "            image_paths.append(os.path.join(direct, label, filename))\n",
        "            labels.append(label)\n",
        "\n",
        "            # Check if it's an original image (not translated)\n",
        "            if not filename.endswith(\"_translated.jpg\"):\n",
        "                # Add translated version if it exists\n",
        "                translated_filename = f\"{filename[:-4]}_translated.jpg\"\n",
        "                translated_path = os.path.join(direct, label, translated_filename)\n",
        "                if os.path.exists(translated_path):\n",
        "                    image_paths.append(translated_path)\n",
        "                    labels.append(label)  # Same label as original\n",
        "\n",
        "        print(label, \"completed\")\n",
        "    return image_paths, labels"
      ],
      "metadata": {
        "id": "0Y3VaqJZP2Nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataframe for training set"
      ],
      "metadata": {
        "id": "HLJBoL5sQ225"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "id": "nRztMTNPRbes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.DataFrame()\n",
        "train['image'], train['label'] = dataframe(train_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xgcRPSbP-_5",
        "outputId": "ab42abd8-eb91-4063-dd28-d67ecd2a4525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral completed\n",
            "disgust completed\n",
            "sad completed\n",
            "fear completed\n",
            "surprise completed\n",
            "happy completed\n",
            "angry completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create dataframe for testing set"
      ],
      "metadata": {
        "id": "-JXp1AtnQ6GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.DataFrame()\n",
        "test['image'], test['label'] = dataframe(test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrLsbm0gQDtw",
        "outputId": "04b320d8-4e2e-4e6a-dabb-dedb8de2a1f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neutral completed\n",
            "disgust completed\n",
            "sad completed\n",
            "fear completed\n",
            "surprise completed\n",
            "happy completed\n",
            "angry completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the translation function on training set"
      ],
      "metadata": {
        "id": "gZcGsxl-RDsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from tqdm import tqdm  # For progress bar\n",
        "\n",
        "# Assuming your training data is stored in a DataFrame called 'train'\n",
        "# with columns 'image' and 'label'\n",
        "\n",
        "# Get the count of samples in each class\n",
        "label_counts = train['label'].value_counts()\n",
        "\n",
        "# Find the maximum class size\n",
        "max_class_size = label_counts.max()\n",
        "\n",
        "print(f\"Maximum class size: {max_class_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pGVW-6aRq77",
        "outputId": "14e4a0fa-9c77-4e3c-ab34-de1dc7e83674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum class size: 7164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_image(image, shift_x, shift_y):\n",
        "    \"\"\"Translates an image by the given shift values.\"\"\"\n",
        "    M = np.float32([[1, 0, shift_x], [0, 1, shift_y]])\n",
        "    translated_image = cv2.warpAffine(image, M, (image.shape[1], image.shape[0]))\n",
        "    return translated_image\n",
        "\n",
        "training_dir = \"/content/face-expression-recognition-dataset/images/train\"  # Your training data path\n",
        "shift_range = 5  # Maximum translation in pixels (adjust as needed)\n",
        "\n",
        "for emotion_folder in os.listdir(training_dir):\n",
        "    emotion_path = os.path.join(training_dir, emotion_folder)\n",
        "    num_samples = len(os.listdir(emotion_path))  # Current samples in class\n",
        "\n",
        "    # Calculate how many more samples are needed\n",
        "    samples_needed = max_class_size - num_samples\n",
        "\n",
        "    if samples_needed > 0:  # Augment if needed\n",
        "        print(f\"Augmenting {emotion_folder} with {samples_needed} samples\")\n",
        "\n",
        "        # Get a list of images in the current class\n",
        "        image_files = os.listdir(emotion_path)\n",
        "\n",
        "        # Augment and save new images\n",
        "        for _ in range(samples_needed):\n",
        "            # Randomly select an image to augment\n",
        "            image_file = np.random.choice(image_files)\n",
        "            image_path = os.path.join(emotion_path, image_file)\n",
        "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "            # Generate random translation values\n",
        "            shift_x = np.random.randint(-shift_range, shift_range + 1)\n",
        "            shift_y = np.random.randint(-shift_range, shift_range + 1)\n",
        "\n",
        "            # Translate the image\n",
        "            translated_image = translate_image(image, shift_x, shift_y)\n",
        "\n",
        "            # Save the augmented image\n",
        "            new_filename = f\"{image_file[:-4]}_translated_{_}.jpg\"  # Unique name\n",
        "            new_image_path = os.path.join(emotion_path, new_filename)\n",
        "            cv2.imwrite(new_image_path, translated_image)\n",
        "\n",
        "print(\"Data augmentation completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vV_z65TvQK8Y",
        "outputId": "62fc37a4-f2f3-4a12-cf4c-5d301587ff51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Augmenting neutral with 2182 samples\n",
            "Augmenting disgust with 6728 samples\n",
            "Augmenting sad with 2226 samples\n",
            "Augmenting fear with 3061 samples\n",
            "Augmenting surprise with 3959 samples\n",
            "Augmenting angry with 3171 samples\n",
            "Data augmentation completed.\n"
          ]
        }
      ]
    }
  ]
}